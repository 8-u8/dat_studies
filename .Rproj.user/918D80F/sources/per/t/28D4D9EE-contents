---
title: "DAT LB1 メモ"
author: "Kien Knot"
date: "2019/11/6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## はじめに  
講義のメモです。  
講義資料にはあるので、Rでの再現実装などをやります。  

## ざっくりメモ。
### 11月5日 午前『GLMからベイズモデリングへ』
#### あらすじ  
- 講師は統計生態学者。モデルのデータが生態学寄り。  
  - うなぎデータやカケスデータの説明をする時すげーたのしそう。
- 基本的に一般化線形モデルの話。
  - 技術的実装ではなく理論、比較的厳密。それでも大まかな内容は
  - 評価指標(Residual Deviancesを手実装してた)

カケスデータにおけるPoisson回帰の例では、特定の植生との関係が示唆されていたが、  
多くの植物がいるとカケスを「見つけること」が難しいことから得られる関係性であり、  
「純粋な生息域のモデル」と「観測可能性のモデル」を明確に分けることは割と難しい話。  
ウチで言えばバーコードでの生活者購買のモデル化などでにたことが言えそうです。  

#### 統計モデリングとは  

現実に得られるデータの生成過程を近似するようなモデルを統計的見地から構築する試み。  
現象を理解したり、予測したり、意思決定に寄与したりすることを目的にする。  
統計モデリングの試みには2方向存在。  

- 観察されるデータから近似される統計モデルを「推測」する
  - 逆問題という
- 近似した予測モデルに基づいてデータを「予測」する  
  - 順問題という  

#### 正規線形モデル  
正規線形モデルとは、例えば2つの確率変数$x$と$y$に対して  

$y = a + bx_i + e_i$

で表現されるようなモデルを指す。  
講義では$a + bx_i$を系統的(決定論)部分、$e_i$を確率的部分に分け、正規線形モデルでは確率的成分$e_i$がすべての$i$で同じパラメータ$(\mu, \sigma)$の正規分布に従うと説明している。  
(なお、この$e_i$を$i$ごとに異なるパラメータの正規分布(多変量正規分布)に従う場合を一般線形モデル(**General** Linear Model)とかいう)

正規線形モデルは誤差項に対する制約が「正規分布に従う」と強い。これを和らげたい。  
系統的部分の仮定は「線型結合」であるが、今回は確率的部分の制約の緩和について議論する。  

#### 確率的成分について  
確率変数や確率分布については色んな本があるのでそっちを参照。  
統計モデリングは経験的な分布(名前がない)を、なんらかの理論分布(名前がある)で表現できないかを探索する。  
これがモデルの当てはめ。

例: プロイセンの馬に蹴られて死んだ兵士のデータを用いた最尤推定  
最尤推定は「データの同時確率を、パラメータ所与の関数で捉えたもの(尤度関数)の最大化」を目指す。  
数式を使うと、パラメータ$\theta$が与えられて決定される確率変数$y$の条件付き確率$p(y|\theta)$を  
「$\theta$についての関数$L(\theta|y)$」として見る。こいつを最大化する。

プロイセンのデータでは、ポアソン分布$Poisson(\lambda)$に従うと考える。つまり  
$L(\lambda|y) = \Pi_{i}^{N}Poisson(y|\lambda)$  
を考える。まあ掛け算とかすると積分などで詰むので、対数尤度  
$log(L(\lambda|y)) = \sum{log(Poisson(y|\lambda))}$  
にしてあげる。

- 資料から読みやすいようにコードは加工している。

```{r echo = TRUE}
library(pscl)
set.seed(123)
Prussian <- pscl::prussian # psclパッケージにあるプロイセンデータ。

# 対数尤度関数を定義。
nloglik <- function(lambda){
  # dpoisはポアソン分布の確率関数。log=TRUEで対数値が入る。
  pois_dens <- dpois(Prussian$y, lambda, log = TRUE) 
  # 尤度に負を設けている
  output    <- -sum(pois_dens)
  return(output)
}
nloglik(1)
nloglik(2)
nloglik(3)

# 最適化optimizeによる最尤推定
result <- optimize(nloglik, interval = c(0, 1e6))
result

liklihood_value <- dpois(0:7, result$minimum) # 尤度を負にしていたので、最小値でおｋ

plot(liklihood_value, type = "b")
```

- 最尤推定の結果、なんか$\lambda = 0.7$がそれっぽいらしい。


一方で、この「それっぽいパラメータ」でどれだけ未知のデータに対して「予測」できるかを考える。  
「予測」の結果(推定値)がどれだけばらつくか？という問題を定式化する。  

```{r echo = TRUE}
bres <- vector(length = 1000)
for(i in 1:1000){
  sample_size <- length(Prussian$y)
  y_rep <- rpois(sample_size, result$minimum)
  
  local_nloglik <- function(lambda){
    pois_dens <- dpois(y_rep, lambda, log = TRUE) 
  # 尤度に負を設けている
    output    <- -sum(pois_dens)
    return(output)
  }
  # 最尤推定
  local_result <- optimize(local_nloglik, interval = c(0,1e6))
  bres[i]      <- local_result$minimum
}

hist(bres, col = "green")              # このヒストグラムが推定値λの分布(ベイズっぽい)
abline(v = result$minimum,col = "red") # 赤い線が元推定値
```

今回は推定値をもとに予測値を「生成」した。これをブートストラップ法とかいう。  

一方で、モデルの当てはまりについて考える。当てはめたモデルがデータの「変動」を適切に表現しているかを検討する。  
扱う指標はresidual deviance。Dobsonだと「尤離度」とか訳されている。残差っぽい。

$D^*(y;\hat{\theta}) = 2\log{L(\theta^*|y)} - 2\log{L(\hat{\theta}|y)}$

$2\log{L(\theta^*|y)}$は、フルモデルと呼ばれる。データの値をそのデータのパラメータにぶち込んだモノ。

$2\log{L(\hat{\theta}|y)}$が、当てはめたモデルの対数尤度

```{r echo = TRUE}
bdev <- vector(length = 1000)
for(i in 1:1000){
  sample_size <- length(Prussian$y)
  y_rep <- rpois(sample_size, result$minimum)
  
  local_nloglik <- function(lambda){
    pois_dens <- dpois(y_rep, lambda, log = TRUE) 
  # 尤度に負を設けている
    output    <- -sum(pois_dens)
    return(output)
  }
  # 最尤推定
  local_result <- optimize(local_nloglik, interval = c(0,1e6))
  bres         <- local_result$minimum
  
  local_full_model <- 2 * sum(dpois(y_rep, y_rep, log = TRUE))
  local_fit_model  <- 2 * sum(dpois(y_rep, bres , log = TRUE))
  bdev[i]          <- local_full_model - local_fit_model
}

global_full_model <- 2 * sum(dpois(Prussian$y, Prussian$y    , log = TRUE))
global_fit_model  <- 2 * sum(dpois(Prussian$y, result$minimum, log = TRUE))
global_deviance   <- global_full_model - global_fit_model

print(global_deviance)

hist(bdev, col = "green")              # このヒストグラムがdevianceの分布
abline(v = global_deviance, col = "red") # 赤い線が元推定値
```

- 結果は、もとのモデルのresidual deviance(赤い線)がヒストグラムの中に入っている
  - モデルが元データの変動を適切に評価していますね。と結論しちゃう。

#### 一般化線形モデル(GLM)

一般化線形モデルは、正規線形モデルの確率的成分を指数型分布族に拡張したモデル。

- 指数型分布族？
  - Dobsonに詳しい。二項分布とかPoisson分布とか正規分布とかが含まれる。
  
データの上下限が実数であると限らない場合(例: 0か1のデータ、非負の整数をとるなど)、これをうまいこと連続変量へ変換する試みをする。必要がある。  
これを実現するのが「リンク関数」と呼ばれる変数変換。  

- 正規線形モデルは恒等リンク関数を使った一般化線形モデルとも解釈できる。
  - 恒等リンク関数$f(\mu) = \mu$
- Poisson回帰では対数リンク関数$g(\mu) = \log{\mu}$
- ロジスティック回帰ではロジットリンク関数 $\eta(\mu) = \log{\frac{\mu}{1-\mu}}$ 
  - 二値回帰には他にもプロビットやトービットと呼ばれるリンク関数が知られる。
  - ロジットリンク関数が使われるメリットは、リンク変換が対数オッズとして解釈でき、モデルの説明がしやすいこと。
  
代表的なGLM回帰であるPoisson回帰とロジスティック回帰について考える。

Poisson回帰は「回数」を回帰する時に使う。マーケティング文脈では「単位期間内にある商品を購入した回数」などが典型的である。  
Poisson分布はパラメータ$\lambda$によって形状と尺度が決まる(強度と呼ぶこともあるが別にいい)。  
Poisson回帰モデルは、以下のようなモデリングが行われる。すなわち、目的変数$y_i$と説明変数$x_i$があって  
$y_i \sim Poisson(\lambda), $  
$\lambda = \log(\beta_0 + \sum_{j=1}^p\beta_jx_j)$  

で表現されるモデル。

- プロイセンデータでの例  
切片回帰を試みる。理屈上、上記で示した値(0.7)と近似される。  
```{r echo = TRUE}
poisson_reg <- glm(y ~ 1, data = Prussian, family = "poisson")
summary(poisson_reg) # 逆リンク変換してません。
exp(poisson_reg$coefficient) # 逆リンク変換した。
```

- 結果、それっぽい。

ロジスティック回帰は、広く知られているように、目的変数が0または1の二値をとるような場合に用いられる。

### 11月5日 午後
#### あらすじ  
伊庭幸人先生。モデリングの鬼。  
階層ベイズモデルなどについて学びます。

#### ベイズ統計入門
- ベイズ統計ってどういう話だ
  - 頻度主義的統計学(推測統計)よりも**仮定**が多い
    - 仮定: 事前分布を要請する


#### MCMC入門
- 高次元確率分布から乱数を創る方法の一つ。
  - 他にもラプラス近似とかで作れる。
  - 変分ベイズとかも言われる。

- 2日目ではカルマンフィルタでの生成や粒子フィルタでの生成も話すよ。

- MCMCは乱数生成アルゴリズムに近い。
  - 収束が一般に遅い。でもstanだといい。

- マルコフ連鎖の例。
  - 2次元正規分布をベースにMCMCで乱数を生成してみる。
  - ギブスサンプリングによる乱数生成です。
  
```{r echo =TRUE}
# 初期値
b <- 0.8
x <- 3.0
y <- 9.0

i_max <- 100
set.seed(1234)
# 再現してみる。

plot(0,0, xlim = c(-10,10), ylim = c(-10,10),type = "n")
title(c(paste("Gibbs sampler",paste("b=",b))))

for(iter in 1:i_max){
  xold  <- x
  x_new_mean <- b*x
  x    <- rnorm(1, mean = x_new_mean, sd = 1)
  
  lines(c(xold,x),c(y,y), type = "b", col = 4, lwd = 2)
  #scan(stdin())
  
  yold = y
  y_new_mean <- b*y
  y    <- rnorm(1, mean = y_new_mean, sd = 1)
  
  lines(c(x,x),c(yold,y), type = "b", col = 4, lwd = 2)
  #scan(stdin())
  
}

```

- かわいい。
  - なんかどっか行きがちだったりする。
  
- マルコフ連鎖することが重要。扱いやすい性質があるので。
  - 定常分布に確率的に収束する。
  - エルゴード性を持つ
  
- MCMCの「収束」って何？
  - 「分布」が収束する。
  - いっぱい移動するけど移動する範囲がある確率分布から外れなくなるイメージ
  
- MCMCによるStanのロジット回帰と、頻度主義的ロジット回帰は推定値がほぼ一致する。
  
#### 階層ベイズモデル入門
- ポアソン分布に近い分布に見えるデータでも、ちょっと違う分布もあったりする。
  - 例: 店舗ユニークになっているデータの売上個数など
  
- このような現象を過分散(Overdispersion)と呼ぶ
  - 裏にあるメカニズムはいろいろあるが、今回は仮定として「グループ差」を考える。
  - グループによってポアソン分布のパラメータ$\lambda$が異なると仮定する。

#### ベイズ平滑化
